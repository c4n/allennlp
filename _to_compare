[1mdiff --git a/allennlp/interpret/attackers/__init__.py b/allennlp/interpret/attackers/__init__.py[m
[1mindex 8255e75..b0240f7 100644[m
[1m--- a/allennlp/interpret/attackers/__init__.py[m
[1m+++ b/allennlp/interpret/attackers/__init__.py[m
[36m@@ -1,3 +1,4 @@[m
 from allennlp.interpret.attackers.attacker import Attacker[m
 from allennlp.interpret.attackers.input_reduction import InputReduction[m
 from allennlp.interpret.attackers.hotflip import Hotflip[m
[32m+[m[32mfrom allennlp.interpret.attackers.myhotflip import MyHotflip[m
[1mdiff --git a/allennlp/interpret/attackers/blackbox.py b/allennlp/interpret/attackers/blackbox.py[m
[1mnew file mode 100644[m
[1mindex 0000000..6f6a6ad[m
[1m--- /dev/null[m
[1m+++ b/allennlp/interpret/attackers/blackbox.py[m
[36m@@ -0,0 +1,11 @@[m
[32m+[m[32mfrom allennlp.interpret.attackers.attacker import Attacker[m
[32m+[m[32mfrom allennlp.predictors import Predictor[m
[32m+[m
[32m+[m
[32m+[m[32m@Attacker.register("blackbox")[m
[32m+[m[32mclass BlackBox(Attacker):[m
[32m+[m[32m    """[m
[32m+[m[32m    """[m
[32m+[m
[32m+[m[32m    def __init__(self, predictor: Predictor) -> None:[m
[32m+[m[32m        super().__init__(predictor)[m
[1mdiff --git a/allennlp/interpret/attackers/myhotflip.py b/allennlp/interpret/attackers/myhotflip.py[m
[1mnew file mode 100644[m
[1mindex 0000000..ffaec89[m
[1m--- /dev/null[m
[1m+++ b/allennlp/interpret/attackers/myhotflip.py[m
[36m@@ -0,0 +1,265 @@[m
[32m+[m[32mfrom copy import deepcopy[m
[32m+[m[32mfrom typing import Dict, List[m
[32m+[m
[32m+[m[32mimport numpy[m
[32m+[m[32mimport torch[m
[32m+[m[32mimport random[m
[32m+[m
[32m+[m[32mfrom allennlp.common.util import JsonDict, sanitize[m
[32m+[m[32mfrom allennlp.data.fields import TextField[m
[32m+[m[32mfrom allennlp.data.token_indexers import ([m
[32m+[m[32m    ELMoTokenCharactersIndexer,[m
[32m+[m[32m    TokenCharactersIndexer,[m
[32m+[m[32m    SingleIdTokenIndexer,[m
[32m+[m[32m)[m
[32m+[m[32mfrom allennlp.data.tokenizers import Token[m
[32m+[m[32mfrom allennlp.interpret.attackers import utils[m
[32m+[m[32mfrom allennlp.interpret.attackers import Hotflip[m
[32m+[m[32mfrom allennlp.data import Instance[m
[32m+[m
[32m+[m[32mfrom allennlp.nn import util[m
[32m+[m
[32m+[m[32mDEFAULT_IGNORE_TOKENS = ["@@NULL@@", ".", ",", ";", "!", "?", "[MASK]", "[SEP]", "[CLS]"][m
[32m+[m
[32m+[m
[32m+[m[32m# @Attacker.register("my_hotflip")[m
[32m+[m[32mclass MyHotflip(Hotflip):[m
[32m+[m[32m    """[m
[32m+[m[32m    Runs the HotFlip style attack at the word-level https://arxiv.org/abs/1712.06751.  We use the[m
[32m+[m[32m    first-order taylor approximation described in https://arxiv.org/abs/1903.06620, in the function[m
[32m+[m[32m    ``_first_order_taylor()``.[m
[32m+[m
[32m+[m[32m    We try to re-use the embedding matrix from the model when deciding what other words to flip a[m
[32m+[m[32m    token to.  For a large class of models, this is straightforward.  When there is a[m
[32m+[m[32m    character-level encoder, however (e.g., with ELMo, any char-CNN, etc.), or a combination of[m
[32m+[m[32m    encoders (e.g., ELMo + glove), we need to construct a fake embedding matrix that we can use in[m
[32m+[m[32m    ``_first_order_taylor()``.  We do this by getting a list of words from the model's vocabulary[m
[32m+[m[32m    and embedding them using the encoder.  This can be expensive, both in terms of time and memory[m
[32m+[m[32m    usage, so we take a ``max_tokens`` parameter to limit the size of this fake embedding matrix.[m
[32m+[m[32m    This also requires a model to `have` a token vocabulary in the first place, which can be[m
[32m+[m[32m    problematic for models that only have character vocabularies.[m
[32m+[m
[32m+[m[32m    Parameters[m
[32m+[m[32m    ----------[m
[32m+[m[32m    predictor : ``Predictor``[m
[32m+[m[32m        The model (inside a Predictor) that we're attacking.  We use this to get gradients and[m
[32m+[m[32m        predictions.[m
[32m+[m[32m    vocab_namespace : ``str``, optional (default='tokens')[m
[32m+[m[32m        We use this to know three things: (1) which tokens we should ignore when producing flips[m
[32m+[m[32m        (we don't consider non-alphanumeric tokens); (2) what the string value is of the token that[m
[32m+[m[32m        we produced, so we can show something human-readable to the user; and (3) if we need to[m
[32m+[m[32m        construct a fake embedding matrix, we use the tokens in the vocabulary as flip candidates.[m
[32m+[m[32m    max_tokens : ``int``, optional (default=5000)[m
[32m+[m[32m        This is only used when we need to construct a fake embedding matrix.  That matrix can take[m
[32m+[m[32m        a lot of memory when the vocab size is large.  This parameter puts a cap on the number of[m
[32m+[m[32m        tokens to use, so the fake embedding matrix doesn't take as much memory.[m
[32m+[m[32m    """[m
[32m+[m
[32m+[m[32m    def _make_embedder_input(self, all_tokens: List[str]) -> Dict[str, torch.Tensor]:[m
[32m+[m[32m        inputs = {}[m
[32m+[m[32m        # A bit of a hack; this will only work with some dataset readers, but it'll do for now.[m
[32m+[m[32m        indexers = self.predictor._dataset_reader._token_indexers  # type: ignore[m
[32m+[m[32m        for indexer_name, token_indexer in indexers.items():[m
[32m+[m[32m            if isinstance(token_indexer, SingleIdTokenIndexer):[m
[32m+[m[32m                all_indices = [[m
[32m+[m[32m                    self.vocab.get_token_index(token, self.namespace) for token in all_tokens[m
[32m+[m[32m                ][m
[32m+[m[32m                inputs[indexer_name] = torch.LongTensor(all_indices).unsqueeze(0)[m
[32m+[m[32m            elif isinstance(token_indexer, TokenCharactersIndexer):[m
[32m+[m[32m                tokens = [Token(x) for x in all_tokens][m
[32m+[m[32m                max_token_length = max(len(x) for x in all_tokens)[m
[32m+[m[32m                # sometime max_token_length is too short for cnn encoder[m
[32m+[m[32m                max_token_length = max(max_token_length, token_indexer._min_padding_length)[m
[32m+[m[32m                indexed_tokens = token_indexer.tokens_to_indices([m
[32m+[m[32m                    tokens, self.vocab, "token_characters"[m
[32m+[m[32m                )[m
[32m+[m[32m                padded_tokens = token_indexer.as_padded_tensor([m
[32m+[m[32m                    indexed_tokens,[m
[32m+[m[32m                    {"token_characters": len(tokens)},[m
[32m+[m[32m                    {"num_token_characters": max_token_length},[m
[32m+[m[32m                )[m
[32m+[m[32m                inputs[indexer_name] = torch.LongTensor([m
[32m+[m[32m                    padded_tokens["token_characters"][m
[32m+[m[32m                ).unsqueeze(0)[m
[32m+[m[32m            elif isinstance(token_indexer, ELMoTokenCharactersIndexer):[m
[32m+[m[32m                elmo_tokens = [][m
[32m+[m[32m                for token in all_tokens:[m
[32m+[m[32m                    elmo_indexed_token = token_indexer.tokens_to_indices([m
[32m+[m[32m                        [Token(text=token)], self.vocab, "sentence"[m
[32m+[m[32m                    )["sentence"][m
[32m+[m[32m                    elmo_tokens.append(elmo_indexed_token[0])[m
[32m+[m[32m                inputs[indexer_name] = torch.LongTensor(elmo_tokens).unsqueeze(0)[m
[32m+[m[32m            else:[m
[32m+[m[32m                raise RuntimeError("Unsupported token indexer:", token_indexer)[m
[32m+[m
[32m+[m[32m        return util.move_to_device(inputs, self.cuda_device)[m
[32m+[m
[32m+[m[32m    def attack_from_instance([m
[32m+[m[32m        self,[m
[32m+[m[32m        instance: Instance,[m
[32m+[m[32m        input_field_to_attack: str = "tokens",[m
[32m+[m[32m        grad_input_field: str = "grad_input_1",[m
[32m+[m[32m        ignore_tokens: List[str] = None,[m
[32m+[m[32m        target: JsonDict = None,[m
[32m+[m[32m    ) -> JsonDict:[m
[32m+[m[32m        """[m
[32m+[m[32m        Replaces one token at a time from the input until the model's prediction changes.[m
[32m+[m[32m        ``input_field_to_attack`` is for example ``tokens``, it says what the input field is[m
[32m+[m[32m        called.  ``grad_input_field`` is for example ``grad_input_1``, which is a key into a grads[m
[32m+[m[32m        dictionary.[m
[32m+[m
[32m+[m[32m        The method computes the gradient w.r.t. the tokens, finds the token with the maximum[m
[32m+[m[32m        gradient (by L2 norm), and replaces it with another token based on the first-order Taylor[m
[32m+[m[32m        approximation of the loss.  This process is iteratively repeated until the prediction[m
[32m+[m[32m        changes.  Once a token is replaced, it is not flipped again.[m
[32m+[m
[32m+[m[32m        Parameters[m
[32m+[m[32m        ----------[m
[32m+[m[32m        instance: ``Instance``,[m
[32m+[m[32m            The model inputs, the same as what is passed to a ``Predictor``.[m
[32m+[m[32m        input_field_to_attack : ``str``, optional (default='tokens')[m
[32m+[m[32m            The field that has the tokens that we're going to be flipping.  This must be a[m
[32m+[m[32m            ``TextField``.[m
[32m+[m[32m        grad_input_field : ``str``, optional (default='grad_input_1')[m
[32m+[m[32m            If there is more than one field that gets embedded in your model (e.g., a question and[m
[32m+[m[32m            a passage, or a premise and a hypothesis), this tells us the key to use to get the[m
[32m+[m[32m            correct gradients.  This selects from the output of :func:`Predictor.get_gradients`.[m
[32m+[m[32m        ignore_tokens : ``List[str]``, optional (default=DEFAULT_IGNORE_TOKENS)[m
[32m+[m[32m            These tokens will not be flipped.  The default list includes some simple punctuation,[m
[32m+[m[32m            OOV and padding tokens, and common control tokens for BERT, etc.[m
[32m+[m[32m        target : ``JsonDict``, optional (default=None)[m
[32m+[m[32m            If given, this will be a `targeted` hotflip attack, where instead of just trying to[m
[32m+[m[32m            change a model's prediction from what it current is predicting, we try to change it to[m
[32m+[m[32m            a `specific` target value.  This is a ``JsonDict`` because it needs to specify the[m
[32m+[m[32m            field name and target value.  For example, for a masked LM, this would be something[m
[32m+[m[32m            like ``{"words": ["she"]}``, because ``"words"`` is the field name, there is one mask[m
[32m+[m[32m            token (hence the list of length one), and we want to change the prediction from[m
[32m+[m[32m            whatever it was to ``"she"``.[m
[32m+[m[32m        """[m
[32m+[m[32m        if self.embedding_matrix is None:[m
[32m+[m[32m            self.initialize()[m
[32m+[m[32m        ignore_tokens = DEFAULT_IGNORE_TOKENS if ignore_tokens is None else ignore_tokens[m
[32m+[m
[32m+[m[32m        # If `target` is `None`, we move away from the current prediction, otherwise we move[m
[32m+[m[32m        # _towards_ the target.[m
[32m+[m[32m        sign = -1 if target is None else 1[m
[32m+[m
[32m+[m[32m        if target is None:[m
[32m+[m[32m            output_dict = self.predictor._model.forward_on_instance(instance)[m
[32m+[m[32m        else:[m
[32m+[m[32m            output_dict = target[m
[32m+[m
[32m+[m[32m        # This now holds the predictions that we want to change (either away from or towards,[m
[32m+[m[32m        # depending on whether `target` was passed).  We'll use this in the loop below to check for[m
[32m+[m[32m        # when we've met our stopping criterion.[m
[32m+[m
[32m+[m[32m        original_instances = self.predictor.predictions_to_labeled_instances(instance, output_dict)[m
[32m+[m[32m        # This is just for ease of access in the UI, so we know the original tokens.  It's not used[m
[32m+[m[32m        # in the logic below.[m
[32m+[m[32m        original_text_field: TextField = original_instances[0][  # type: ignore[m
[32m+[m[32m            input_field_to_attack[m
[32m+[m[32m        ][m
[32m+[m[32m        original_tokens = deepcopy(original_text_field.tokens)[m
[32m+[m
[32m+[m[32m        # if all tags are Os, don't attack.[m
[32m+[m[32m        if all(tag == "O" for tag in output_dict["tags"]):[m
[32m+[m[32m            return sanitize({"final": [original_tokens], "original": original_tokens})[m
[32m+[m
[32m+[m[32m        final_tokens = [][m
[32m+[m[32m        # `original_instances` is a list because there might be several different predictions that[m
[32m+[m[32m        # we're trying to attack (e.g., all of the NER tags for an input sentence).  We attack them[m
[32m+[m[32m        # one at a time.[m
[32m+[m[32m        original_instance = random.choice(original_instances)[m
[32m+[m
[32m+[m[32m        # Gets a list of the fields that we want to check to see if they change.[m
[32m+[m[32m        fields_to_compare = utils.get_fields_to_compare_instance([m
[32m+[m[32m            instance, original_instance, input_field_to_attack[m
[32m+[m[32m        )[m
[32m+[m
[32m+[m[32m        # We'll be modifying the tokens in this text field below, and grabbing the modified[m
[32m+[m[32m        # list after the `while` loop.[m
[32m+[m[32m        text_field: TextField = instance[input_field_to_attack]  # type: ignore[m
[32m+[m
[32m+[m[32m        # Because we can save computation by getting grads and outputs at the same time, we do[m
[32m+[m[32m        # them together at the end of the loop, even though we use grads at the beginning and[m
[32m+[m[32m        # outputs at the end.  This is our initial gradient for the beginning of the loop.  The[m
[32m+[m[32m        # output can be ignored here.[m
[32m+[m[32m        grads, outputs = self.predictor.get_gradients([instance])[m
[32m+[m
[32m+[m[32m        # Ignore any token that is in the ignore_tokens list by setting the token to already[m
[32m+[m[32m        # flipped.[m
[32m+[m[32m        flipped: List[int] = [][m
[32m+[m[32m        for index, token in enumerate(text_field.tokens):[m
[32m+[m[32m            if token.text in ignore_tokens:[m
[32m+[m[32m                flipped.append(index)[m
[32m+[m[32m        if "clusters" in outputs:[m
[32m+[m[32m            # Coref unfortunately needs a special case here.  We don't want to flip words in[m
[32m+[m[32m            # the same predicted coref cluster, but we can't really specify a list of tokens,[m
[32m+[m[32m            # because, e.g., "he" could show up in several different clusters.[m
[32m+[m[32m            # TODO(mattg): perhaps there's a way to get `predictions_to_labeled_instances` to[m
[32m+[m[32m            # return the set of tokens that shouldn't be changed for each instance?  E.g., you[m
[32m+[m[32m            # could imagine setting a field on the `Token` object, that we could then read[m
[32m+[m[32m            # here...[m
[32m+[m[32m            for cluster in outputs["clusters"]:[m
[32m+[m[32m                for mention in cluster:[m
[32m+[m[32m                    for index in range(mention[0], mention[1] + 1):[m
[32m+[m[32m                        flipped.append(index)[m
[32m+[m
[32m+[m[32m        while True:[m
[32m+[m[32m            # Compute L2 norm of all grads.[m
[32m+[m[32m            grad = grads[grad_input_field][0][m
[32m+[m[32m            grads_magnitude = [g.dot(g) for g in grad][m
[32m+[m
[32m+[m[32m            # only flip a token once[m
[32m+[m[32m            for index in flipped:[m
[32m+[m[32m                grads_magnitude[index] = -1[m
[32m+[m
[32m+[m[32m            # We flip the token with highest gradient norm.[m
[32m+[m[32m            index_of_token_to_flip = numpy.argmax(grads_magnitude)[m
[32m+[m[32m            if grads_magnitude[index_of_token_to_flip] == -1:[m
[32m+[m[32m                # If we've already flipped all of the tokens, we give up.[m
[32m+[m[32m                break[m
[32m+[m[32m            flipped.append(index_of_token_to_flip)[m
[32m+[m
[32m+[m[32m            # TODO(mattg): This is quite a bit of a hack for getting the vocab id...  I don't[m
[32m+[m[32m            # have better ideas at the moment, though.[m
[32m+[m[32m            indexer_name = self.namespace[m
[32m+[m[32m            input_tokens = text_field._indexed_tokens[indexer_name][m
[32m+[m[32m            original_id_of_token_to_flip = input_tokens[index_of_token_to_flip][m
[32m+[m
[32m+[m[32m            # Get new token using taylor approximation.[m
[32m+[m[32m            new_id = self._first_order_taylor([m
[32m+[m[32m                grad[index_of_token_to_flip], original_id_of_token_to_flip, sign[m
[32m+[m[32m            )[m
[32m+[m
[32m+[m[32m            # Flip token.  We need to tell the instance to re-index itself, so the text field[m
[32m+[m[32m            # will actually update.[m
[32m+[m[32m            new_token = Token(self.vocab._index_to_token[self.namespace][new_id])  # type: ignore[m
[32m+[m[32m            text_field.tokens[index_of_token_to_flip] = new_token[m
[32m+[m[32m            instance.indexed = False[m
[32m+[m
[32m+[m[32m            # Get model predictions on instance, and then label the instances[m
[32m+[m[32m            grads, outputs = self.predictor.get_gradients([instance])  # predictions[m
[32m+[m[32m            for key, output in outputs.items():[m
[32m+[m[32m                if isinstance(output, torch.Tensor):[m
[32m+[m[32m                    outputs[key] = output.detach().cpu().numpy().squeeze()[m
[32m+[m[32m                elif isinstance(output, list):[m
[32m+[m[32m                    outputs[key] = output[0][m
[32m+[m
[32m+[m[32m            # TODO(mattg): taking the first result here seems brittle, if we're in a case where[m
[32m+[m[32m            # there are multiple predictions.[m
[32m+[m[32m            labeled_instance = self.predictor.predictions_to_labeled_instances(instance, outputs)[0][m
[32m+[m
[32m+[m[32m            # If we've met our stopping criterion, we stop.[m
[32m+[m[32m            has_changed = utils.instance_has_changed(labeled_instance, fields_to_compare)[m
[32m+[m[32m            if target is None and has_changed:[m
[32m+[m[32m                # With no target, we just want to change the prediction.[m
[32m+[m[32m                break[m
[32m+[m[32m            if target is not None and not has_changed:[m
[32m+[m[32m                # With a given target, we want to *match* the target, which we check by[m
[32m+[m[32m                # `not has_changed`.[m
[32m+[m[32m                break[m
[32m+[m
[32m+[m[32m        final_tokens.append(text_field.tokens)[m
[32m+[m
[32m+[m[32m        return sanitize({"final": final_tokens, "original": original_tokens, "outputs": outputs})[m
